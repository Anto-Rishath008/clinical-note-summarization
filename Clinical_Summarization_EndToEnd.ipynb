{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b245ba",
   "metadata": {},
   "source": [
    "# Clinical Note Summarization: End-to-End Pipeline\n",
    "\n",
    "**From-Scratch Hierarchical Pointer-Generator Network with Coverage**\n",
    "\n",
    "This notebook consolidates the entire clinical summarization project into a single, executable pipeline.  \n",
    "All training artifacts are preserved and reused where possible.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã File Consolidation Plan\n",
    "\n",
    "### ‚úÖ **Files KEPT (Active)**\n",
    "- `Clinical_Summarization_EndToEnd.ipynb` ‚Üê **This notebook (primary entry point)**\n",
    "- `train.py` - Training script (can be called from notebook or run standalone)\n",
    "- `evaluate.py` - Evaluation script (can be called from notebook)\n",
    "- `baselines.py` - Baseline comparison scripts\n",
    "- `models/` - Model architecture modules\n",
    "- `utils/` - Dataset, metrics, beam search utilities\n",
    "- `tools/` - Diagnostic and monitoring tools\n",
    "- `configs/` - YAML configuration files\n",
    "- `requirements.txt` - Python dependencies\n",
    "- `README.md` - Project documentation\n",
    "\n",
    "### üì¶ **Artifacts PRESERVED**\n",
    "- `artifacts/tokenizer/spm.model` - Trained SentencePiece tokenizer\n",
    "- `artifacts/checkpoints/final_check/best_model.pt` - Best trained model\n",
    "- `artifacts/checkpoints/final_check/checkpoint_step_500.pt` - Training checkpoint\n",
    "- `data/splits/` - Train/val/test split files\n",
    "- `data/tokenized/subset20000/` - Tokenized parquet shards\n",
    "\n",
    "### üóÑÔ∏è **Files ARCHIVED** (Moved to `archive/`)\n",
    "- `01_data_explore.ipynb` - Exploratory data analysis\n",
    "- `02_small_subset_train.ipynb` - Small-scale training experiments\n",
    "- `03_full_train.ipynb` - Full training notebook\n",
    "- `04_results_and_examples.ipynb` - Results visualization\n",
    "- `infer.py` - Standalone inference (logic moved into evaluate.py)\n",
    "- `sweep.py` - Hyperparameter sweep experiments\n",
    "- `COMMAND_LOG.md`, `EXECUTIVE_SUMMARY.md`, `QA_*.md` - Documentation artifacts\n",
    "\n",
    "### üö´ **NO NEW FILES CREATED**\n",
    "This notebook reuses existing code and artifacts. Future work updates this notebook only.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc6d13",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Project Overview\n",
    "\n",
    "**Goal:** Build a hierarchical pointer-generator network with coverage from scratch to summarize clinical notes.  \n",
    "**Dataset:** MIMIC-IV clinical notes  \n",
    "**Model:** Custom Seq2Seq with bidirectional LSTM encoder, attentional decoder, pointer-generator, and coverage mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae98ee",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Environment Setup\n",
    "\n",
    "**Platform:** Windows 11 with NVIDIA GPU  \n",
    "**Environment:** Python virtual environment (`.venv`)  \n",
    "**Expected Runtime:** ~10 seconds\n",
    "\n",
    "**What this cell does:**\n",
    "- Installs required packages from `requirements.txt`\n",
    "- Imports core libraries\n",
    "- Sets random seeds for reproducibility\n",
    "\n",
    "**Common Issues:**\n",
    "- If CUDA not found: Ensure PyTorch with CUDA is installed (`torch.cuda.is_available()`)\n",
    "- If sentencepiece import fails: Run `pip install sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì Python: {sys.version.split()[0]}\")\n",
    "print(f\"‚úì PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úì NumPy: {np.__version__}\")\n",
    "print(f\"‚úì Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27b187",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ GPU Verification\n",
    "\n",
    "**Expected Runtime:** ~5 seconds  \n",
    "**What this cell does:**\n",
    "- Checks if CUDA is available\n",
    "- Displays GPU name and memory\n",
    "- Runs a small matrix multiplication test to verify GPU compute\n",
    "\n",
    "**Common Issues:**\n",
    "- CUDA unavailable: Check NVIDIA drivers and PyTorch CUDA installation\n",
    "- Low memory: Close other GPU applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Test GPU compute\n",
    "    print(\"\\nüß™ Running GPU compute test...\")\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = torch.randn(1000, 1000, device=device)\n",
    "    z = torch.matmul(x, y)\n",
    "    print(f\"‚úì Matrix multiplication successful: {z.shape}\")\n",
    "    print(f\"‚úì Result sample: {z[0, 0].item():.4f}\")\n",
    "    \n",
    "    # Memory check\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"\\nMemory Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Memory Reserved: {reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available. Training will be VERY slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7d59b",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Dataset Path Configuration\n",
    "\n",
    "**Expected Runtime:** <1 second  \n",
    "**What this cell does:**\n",
    "- Defines paths to dataset, splits, tokenized data, artifacts\n",
    "- Validates that critical files exist\n",
    "\n",
    "**Inputs:**\n",
    "- `DATA_ROOT`: Path to your MIMIC clinical notes CSV\n",
    "- Update this path to match your dataset location\n",
    "\n",
    "**Common Issues:**\n",
    "- FileNotFoundError: Update `DATA_ROOT` to your actual dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURE THESE PATHS ==========\n",
    "# Update DATA_ROOT to your dataset location\n",
    "DATA_ROOT = Path(r\"C:\\Users\\antor\\Desktop\\mimic_clinical_notes.csv\")  # Change this!\n",
    "\n",
    "# Project paths (relative to notebook)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "SPLITS_DIR = DATA_DIR / \"splits\"\n",
    "TOKENIZED_DIR = DATA_DIR / \"tokenized\" / \"subset20000\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "TOKENIZER_DIR = ARTIFACTS_DIR / \"tokenizer\"\n",
    "CHECKPOINTS_DIR = ARTIFACTS_DIR / \"checkpoints\" / \"final_check\"\n",
    "LOGS_DIR = ARTIFACTS_DIR / \"logs\"\n",
    "PREDICTIONS_DIR = ARTIFACTS_DIR / \"predictions\"\n",
    "\n",
    "# Config file\n",
    "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"small_gpu.yaml\"\n",
    "\n",
    "# ========== VALIDATION ==========\n",
    "print(\"üìÅ Path Validation:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Dataset: {DATA_ROOT} [{'‚úì EXISTS' if DATA_ROOT.exists() else '‚úó NOT FOUND'}]\")\n",
    "print(f\"  Tokenizer: {TOKENIZER_DIR / 'spm.model'} [{'‚úì EXISTS' if (TOKENIZER_DIR / 'spm.model').exists() else '‚úó NOT FOUND'}]\")\n",
    "print(f\"  Checkpoint: {CHECKPOINTS_DIR / 'best_model.pt'} [{'‚úì EXISTS' if (CHECKPOINTS_DIR / 'best_model.pt').exists() else '‚úó NOT FOUND'}]\")\n",
    "print(f\"  Tokenized Data: {TOKENIZED_DIR} [{'‚úì EXISTS' if TOKENIZED_DIR.exists() else '‚úó NOT FOUND'}]\")\n",
    "\n",
    "# Create missing directories\n",
    "for dir_path in [DATA_DIR, SPLITS_DIR, ARTIFACTS_DIR, LOGS_DIR, PREDICTIONS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n‚úì Directory structure validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b75f9",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Data Loading & Exploration\n",
    "\n",
    "**Expected Runtime:** ~30 seconds (for 20K subset)  \n",
    "**What this cell does:**\n",
    "- Loads dataset in streaming/chunked mode (memory-efficient)\n",
    "- Shows dataset statistics\n",
    "- Displays sample clinical note + summary\n",
    "\n",
    "**Memory:** Processes in chunks to avoid loading full dataset into RAM\n",
    "\n",
    "**Common Issues:**\n",
    "- CSV encoding errors: Dataset should be UTF-8 encoded\n",
    "- Missing columns: Expects `note_id`, `text`, `summary` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e647a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (streaming to avoid memory issues)\n",
    "print(\"üìä Loading dataset...\")\n",
    "\n",
    "if DATA_ROOT.exists():\n",
    "    # Read first 5 rows to check structure\n",
    "    df_sample = pd.read_csv(DATA_ROOT, nrows=5)\n",
    "    print(f\"\\nColumns: {df_sample.columns.tolist()}\")\n",
    "    print(f\"Sample shape: {df_sample.shape}\")\n",
    "    \n",
    "    # Get dataset size efficiently\n",
    "    chunk_iterator = pd.read_csv(DATA_ROOT, chunksize=10000)\n",
    "    total_rows = sum(len(chunk) for chunk in chunk_iterator)\n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Sample Clinical Note:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Note ID: {df_sample.iloc[0]['note_id']}\")\n",
    "    print(f\"\\nText (first 500 chars):\\n{df_sample.iloc[0]['text'][:500]}...\")\n",
    "    print(f\"\\nSummary:\\n{df_sample.iloc[0]['summary']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nText length (chars): {len(df_sample.iloc[0]['text'])}\")\n",
    "    print(f\"Summary length (chars): {len(df_sample.iloc[0]['summary'])}\")\n",
    "    print(f\"Compression ratio: {len(df_sample.iloc[0]['text']) / len(df_sample.iloc[0]['summary']):.1f}x\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset not found. Update DATA_ROOT path in previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e6673",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Train/Val/Test Split Verification\n",
    "\n",
    "**Expected Runtime:** <5 seconds  \n",
    "**What this cell does:**\n",
    "- Checks if train/val/test splits already exist\n",
    "- Validates split sizes\n",
    "- Option to regenerate splits if needed\n",
    "\n",
    "**Skip Condition:** `RUN_SPLIT = False` (default, use existing splits)\n",
    "\n",
    "**Outputs:**\n",
    "- `data/splits/train.csv`\n",
    "- `data/splits/val.csv`\n",
    "- `data/splits/test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "RUN_SPLIT = False  # Set True to regenerate splits\n",
    "\n",
    "# ========== CHECK EXISTING SPLITS ==========\n",
    "train_path = SPLITS_DIR / \"train.csv\"\n",
    "val_path = SPLITS_DIR / \"val.csv\"\n",
    "test_path = SPLITS_DIR / \"test.csv\"\n",
    "\n",
    "if train_path.exists() and val_path.exists() and test_path.exists():\n",
    "    print(\"‚úì Existing splits found:\")\n",
    "    train_size = sum(1 for _ in open(train_path)) - 1\n",
    "    val_size = sum(1 for _ in open(val_path)) - 1\n",
    "    test_size = sum(1 for _ in open(test_path)) - 1\n",
    "    \n",
    "    print(f\"  Train: {train_size:,} examples\")\n",
    "    print(f\"  Val:   {val_size:,} examples\")\n",
    "    print(f\"  Test:  {test_size:,} examples\")\n",
    "    print(f\"  Total: {train_size + val_size + test_size:,} examples\")\n",
    "    print(f\"\\n  Split ratio: {train_size/(train_size+val_size+test_size)*100:.1f}% / \"\n",
    "          f\"{val_size/(train_size+val_size+test_size)*100:.1f}% / \"\n",
    "          f\"{test_size/(train_size+val_size+test_size)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Splits not found. Set RUN_SPLIT = True to generate.\")\n",
    "\n",
    "# ========== REGENERATE SPLITS (if needed) ==========\n",
    "if RUN_SPLIT:\n",
    "    print(\"\\nüîÑ Regenerating splits...\")\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"data/02_make_splits.py\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    else:\n",
    "        print(\"‚úì Splits regenerated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ec0de",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Train SentencePiece Tokenizer\n",
    "\n",
    "**Expected Runtime:** ~5 minutes (if training from scratch)  \n",
    "**What this cell does:**\n",
    "- Checks if tokenizer already exists\n",
    "- If not, trains a SentencePiece BPE tokenizer on clinical text\n",
    "- Saves tokenizer to `artifacts/tokenizer/spm.model`\n",
    "\n",
    "**Skip Condition:** `RUN_TOKENIZER = False` (default, use existing tokenizer)\n",
    "\n",
    "**Parameters:**\n",
    "- Vocab size: 32,000\n",
    "- Model type: BPE (Byte Pair Encoding)\n",
    "\n",
    "**Common Issues:**\n",
    "- OOM during training: Reduce vocab size or input sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "RUN_TOKENIZER = False  # Set True to retrain tokenizer\n",
    "\n",
    "# ========== CHECK EXISTING TOKENIZER ==========\n",
    "tokenizer_model_path = TOKENIZER_DIR / \"spm.model\"\n",
    "tokenizer_vocab_path = TOKENIZER_DIR / \"spm.vocab\"\n",
    "\n",
    "if tokenizer_model_path.exists():\n",
    "    print(f\"‚úì Tokenizer found: {tokenizer_model_path}\")\n",
    "    \n",
    "    # Load and test tokenizer\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(str(tokenizer_model_path))\n",
    "    \n",
    "    print(f\"  Vocab size: {sp.vocab_size():,}\")\n",
    "    print(f\"  PAD token: {sp.id_to_piece(sp.pad_id())} (ID: {sp.pad_id()})\")\n",
    "    print(f\"  UNK token: {sp.id_to_piece(sp.unk_id())} (ID: {sp.unk_id()})\")\n",
    "    print(f\"  BOS token: {sp.id_to_piece(sp.bos_id())} (ID: {sp.bos_id()})\")\n",
    "    print(f\"  EOS token: {sp.id_to_piece(sp.eos_id())} (ID: {sp.eos_id()})\")\n",
    "    \n",
    "    # Test encoding/decoding\n",
    "    test_text = \"The patient presents with chest pain and shortness of breath.\"\n",
    "    encoded = sp.encode(test_text)\n",
    "    decoded = sp.decode(encoded)\n",
    "    print(f\"\\n  Test encoding:\")\n",
    "    print(f\"    Original: {test_text}\")\n",
    "    print(f\"    Encoded:  {encoded[:10]}... ({len(encoded)} tokens)\")\n",
    "    print(f\"    Decoded:  {decoded}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tokenizer not found. Set RUN_TOKENIZER = True to train.\")\n",
    "\n",
    "# ========== TRAIN TOKENIZER (if needed) ==========\n",
    "if RUN_TOKENIZER:\n",
    "    print(\"\\nüîÑ Training SentencePiece tokenizer...\")\n",
    "    TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"data/03_train_sentencepiece.py\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    else:\n",
    "        print(f\"‚úì Tokenizer saved to {TOKENIZER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f201f",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Tokenization to Parquet Shards\n",
    "\n",
    "**Expected Runtime:** ~10-30 minutes (for full dataset)  \n",
    "**What this cell does:**\n",
    "- Tokenizes train/val/test splits using SentencePiece\n",
    "- Saves to memory-efficient Parquet format\n",
    "- Shards data for faster loading during training\n",
    "\n",
    "**Skip Condition:** `RUN_TOKENIZATION = False` (default, use existing tokenized data)\n",
    "\n",
    "**Outputs:**\n",
    "- `data/tokenized/subset20000/train.parquet`\n",
    "- `data/tokenized/subset20000/val.parquet`\n",
    "- `data/tokenized/subset20000/test.parquet`\n",
    "\n",
    "**Common Issues:**\n",
    "- OOM: Process in smaller chunks (adjust `chunk_size` in tokenization script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "RUN_TOKENIZATION = False  # Set True to retokenize\n",
    "\n",
    "# ========== CHECK EXISTING TOKENIZED DATA ==========\n",
    "tokenized_train = TOKENIZED_DIR / \"train.parquet\"\n",
    "tokenized_val = TOKENIZED_DIR / \"val.parquet\"\n",
    "tokenized_test = TOKENIZED_DIR / \"test.parquet\"\n",
    "\n",
    "if tokenized_train.exists() and tokenized_val.exists() and tokenized_test.exists():\n",
    "    print(\"‚úì Tokenized data found:\")\n",
    "    \n",
    "    # Read metadata\n",
    "    train_df = pd.read_parquet(tokenized_train)\n",
    "    val_df = pd.read_parquet(tokenized_val)\n",
    "    test_df = pd.read_parquet(tokenized_test)\n",
    "    \n",
    "    print(f\"  Train: {len(train_df):,} examples\")\n",
    "    print(f\"  Val:   {len(val_df):,} examples\")\n",
    "    print(f\"  Test:  {len(test_df):,} examples\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\n  Sample tokenized example:\")\n",
    "    sample = train_df.iloc[0]\n",
    "    print(f\"    Note ID: {sample['note_id']}\")\n",
    "    print(f\"    Source tokens: {len(sample['source_ids'])} tokens\")\n",
    "    print(f\"    Target tokens: {len(sample['target_ids'])} tokens\")\n",
    "    print(f\"    Source IDs (first 20): {sample['source_ids'][:20]}\")\n",
    "    print(f\"    Target IDs (first 20): {sample['target_ids'][:20]}\")\n",
    "    \n",
    "    # File sizes\n",
    "    train_size = tokenized_train.stat().st_size / 1e6\n",
    "    val_size = tokenized_val.stat().st_size / 1e6\n",
    "    test_size = tokenized_test.stat().st_size / 1e6\n",
    "    print(f\"\\n  File sizes:\")\n",
    "    print(f\"    Train: {train_size:.1f} MB\")\n",
    "    print(f\"    Val:   {val_size:.1f} MB\")\n",
    "    print(f\"    Test:  {test_size:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tokenized data not found. Set RUN_TOKENIZATION = True.\")\n",
    "\n",
    "# ========== TOKENIZE DATA (if needed) ==========\n",
    "if RUN_TOKENIZATION:\n",
    "    print(\"\\nüîÑ Tokenizing data...\")\n",
    "    TOKENIZED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"data/04_tokenize_to_parquet.py\",\n",
    "         \"--output_dir\", str(TOKENIZED_DIR)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    else:\n",
    "        print(f\"‚úì Tokenized data saved to {TOKENIZED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15a262",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Model Architecture\n",
    "\n",
    "**Expected Runtime:** ~5 seconds  \n",
    "**What this cell does:**\n",
    "- Loads config file\n",
    "- Instantiates PointerGeneratorModel from scratch\n",
    "- Displays model summary and parameter count\n",
    "\n",
    "**Architecture:**\n",
    "- **Encoder:** 2-layer BiLSTM (512 hidden units)\n",
    "- **Decoder:** 2-layer LSTM with additive attention (512 hidden units)\n",
    "- **Pointer-Generator:** Learns when to copy from source vs generate\n",
    "- **Coverage:** Tracks attention history to reduce repetition\n",
    "\n",
    "**Parameters:** ~50-60M (trained from random initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "print(f\"üìã Loading config from {CONFIG_PATH}...\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(yaml.dump(config['model'], default_flow_style=False, indent=2))\n",
    "\n",
    "# Import model\n",
    "from models.model import PointerGeneratorModel\n",
    "\n",
    "# Create model\n",
    "print(\"\\nüèóÔ∏è Building model architecture...\")\n",
    "model = PointerGeneratorModel(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model created successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size (MB): {total_params * 4 / 1e6:.2f}\")\n",
    "\n",
    "# Display model structure\n",
    "print(f\"\\nModel Structure:\")\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nüß™ Testing forward pass...\")\n",
    "batch_size = 2\n",
    "src_len = config['model']['chunk_len'] * config['model']['num_chunks']\n",
    "tgt_len = 64\n",
    "\n",
    "dummy_src = torch.randint(0, config['model']['vocab_size'], (batch_size, src_len)).to(device)\n",
    "dummy_tgt = torch.randint(0, config['model']['vocab_size'], (batch_size, tgt_len)).to(device)\n",
    "dummy_src_ext = dummy_src.clone()\n",
    "dummy_oov_list = [[]] * batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss, _ = model(dummy_src, dummy_tgt, dummy_src_ext, dummy_oov_list)\n",
    "\n",
    "print(f\"‚úì Forward pass successful\")\n",
    "print(f\"  Loss shape: {loss.shape}\")\n",
    "print(f\"  Loss value: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88827e0",
   "metadata": {},
   "source": [
    "## üîü Training\n",
    "\n",
    "**Expected Runtime:** ~6-12 hours (for 10K steps on RTX 4070)  \n",
    "**What this cell does:**\n",
    "- Trains the model using FP16 mixed precision\n",
    "- Saves checkpoints every N steps\n",
    "- Evaluates on validation set periodically\n",
    "- Can resume from existing checkpoint\n",
    "\n",
    "**Skip Condition:** `RUN_TRAIN = False` (default, use existing checkpoint)\n",
    "\n",
    "**Training Features:**\n",
    "- FP16 mixed precision (faster, less memory)\n",
    "- Gradient accumulation (effective batch size = batch_size √ó accum_steps)\n",
    "- Learning rate warmup + decay\n",
    "- Gradient clipping (prevents exploding gradients)\n",
    "\n",
    "**Checkpoints Saved:**\n",
    "- Every 500 steps: `checkpoint_step_<N>.pt`\n",
    "- Best validation ROUGE: `best_model.pt`\n",
    "\n",
    "**Common Issues:**\n",
    "- OOM: Reduce batch_size in config or enable gradient checkpointing\n",
    "- Slow training: Check GPU utilization (should be >80%)\n",
    "- NaN loss: Reduce learning rate or check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759341be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "RUN_TRAIN = False  # Set True to train (or resume training)\n",
    "RESUME_FROM_CHECKPOINT = True  # Resume from existing checkpoint if available\n",
    "MAX_STEPS = 10000  # Total training steps\n",
    "\n",
    "# ========== CHECK EXISTING CHECKPOINT ==========\n",
    "checkpoint_path = CHECKPOINTS_DIR / \"best_model.pt\"\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"‚úì Checkpoint found: {checkpoint_path}\")\n",
    "    \n",
    "    # Load checkpoint metadata\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "    print(f\"  Step: {checkpoint.get('step', 'N/A')}\")\n",
    "    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Best ROUGE-L: {checkpoint.get('best_rouge', 0):.4f}\")\n",
    "    print(f\"  Model params: {sum(p.numel() for p in checkpoint['model_state_dict'].values()):,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No checkpoint found. Training will start from scratch.\")\n",
    "\n",
    "# ========== TRAIN (if enabled) ==========\n",
    "if RUN_TRAIN:\n",
    "    print(\"\\nüöÄ Starting training...\")\n",
    "    print(f\"  Max steps: {MAX_STEPS}\")\n",
    "    print(f\"  Resume: {RESUME_FROM_CHECKPOINT}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Config: {CONFIG_PATH}\")\n",
    "    \n",
    "    # Build training command\n",
    "    train_cmd = [\n",
    "        sys.executable, \"train.py\",\n",
    "        \"--config\", str(CONFIG_PATH),\n",
    "        \"--tokenized_dir\", str(TOKENIZED_DIR),\n",
    "        \"--run_name\", \"notebook_training\",\n",
    "        \"--max_steps\", str(MAX_STEPS)\n",
    "    ]\n",
    "    \n",
    "    if RESUME_FROM_CHECKPOINT and checkpoint_path.exists():\n",
    "        train_cmd.extend([\"--resume\", str(checkpoint_path)])\n",
    "    \n",
    "    # Run training\n",
    "    import subprocess\n",
    "    print(f\"\\nCommand: {' '.join(train_cmd)}\\n\")\n",
    "    \n",
    "    process = subprocess.Popen(\n",
    "        train_cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    # Stream output\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(\"\\n‚úì Training completed successfully\")\n",
    "    else:\n",
    "        print(f\"\\n‚úó Training failed with exit code {process.returncode}\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è Training skipped (RUN_TRAIN = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2b342",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Load Trained Model\n",
    "\n",
    "**Expected Runtime:** ~5 seconds  \n",
    "**What this cell does:**\n",
    "- Loads the best trained checkpoint\n",
    "- Restores model weights\n",
    "- Verifies model is ready for inference\n",
    "\n",
    "**Checkpoint Used:** `artifacts/checkpoints/final_check/best_model.pt`\n",
    "\n",
    "**Common Issues:**\n",
    "- Checkpoint mismatch: Ensure config matches training config\n",
    "- Missing checkpoint: Train model first (set RUN_TRAIN = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fcced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint_path = CHECKPOINTS_DIR / \"best_model.pt\"\n",
    "\n",
    "if not checkpoint_path.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "print(f\"üì¶ Loading checkpoint: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Recreate model (in case not already created)\n",
    "from models.model import PointerGeneratorModel\n",
    "model = PointerGeneratorModel(config).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Model loaded successfully\")\n",
    "print(f\"  Training step: {checkpoint.get('step', 'N/A')}\")\n",
    "print(f\"  Training epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"  Best validation ROUGE-L: {checkpoint.get('best_rouge', 0):.4f}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Mode: Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b584577",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Decoding + Prediction Dump\n",
    "\n",
    "**Expected Runtime:** ~30-60 minutes (for full validation set)  \n",
    "**What this cell does:**\n",
    "- Runs beam search decoding on validation set\n",
    "- Generates summaries for all examples\n",
    "- Saves predictions to JSONL/CSV format\n",
    "\n",
    "**Outputs:**\n",
    "- `artifacts/predictions/val_predictions.csv` (note_id, prediction, reference)\n",
    "\n",
    "**Beam Search Parameters:**\n",
    "- Beam size: 4 (explores 4 candidate sequences)\n",
    "- Max length: 128 tokens\n",
    "- Length penalty: Prevents overly short summaries\n",
    "\n",
    "**Common Issues:**\n",
    "- Slow inference: Reduce beam_size or max_length\n",
    "- OOM: Reduce batch_size to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c150d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "from utils.dataset import get_dataloader\n",
    "from utils.beam_search import beam_search\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.load(str(TOKENIZER_DIR / 'spm.model'))\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size():,}\")\n",
    "\n",
    "# Create validation dataloader\n",
    "print(\"\\nüìä Loading validation data...\")\n",
    "val_dataloader = get_dataloader(\n",
    "    str(TOKENIZED_DIR / 'val.parquet'),\n",
    "    batch_size=1,  # Beam search works on single examples\n",
    "    shuffle=False,\n",
    "    max_src_len=config['model']['chunk_len'] * config['model']['num_chunks'],\n",
    "    max_tgt_len=config['model']['max_target_len'],\n",
    "    pad_id=config['data']['pad_id']\n",
    ")\n",
    "\n",
    "print(f\"  Validation examples: {len(val_dataloader)}\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\nüîÆ Generating predictions...\")\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(val_dataloader, desc=\"Decoding\")):\n",
    "        # Move to device\n",
    "        src_ids = batch['source_ids'].to(device)\n",
    "        src_ext_ids = batch['source_ext_ids'].to(device)\n",
    "        tgt_ids = batch['target_ids'].to(device)\n",
    "        oov_list = batch['oov_list']\n",
    "        note_ids = batch['note_id']\n",
    "        \n",
    "        # Run beam search\n",
    "        pred_ids = beam_search(\n",
    "            model=model,\n",
    "            src_ids=src_ids[0],  # Batch size = 1\n",
    "            src_ext_ids=src_ext_ids[0],\n",
    "            oov_list=oov_list[0],\n",
    "            beam_size=4,\n",
    "            max_len=config['model']['max_target_len'],\n",
    "            bos_id=config['data']['bos_id'],\n",
    "            eos_id=config['data']['eos_id'],\n",
    "            pad_id=config['data']['pad_id'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Decode prediction and reference\n",
    "        pred_text = tokenizer.decode(pred_ids)\n",
    "        ref_ids = tgt_ids[0].cpu().tolist()\n",
    "        ref_text = tokenizer.decode([id for id in ref_ids if id not in [config['data']['pad_id'], config['data']['bos_id'], config['data']['eos_id']]])\n",
    "        \n",
    "        predictions.append({\n",
    "            'note_id': note_ids[0],\n",
    "            'prediction': pred_text,\n",
    "            'reference': ref_text\n",
    "        })\n",
    "        \n",
    "        # Limit to first 100 for demo (remove for full evaluation)\n",
    "        if batch_idx >= 99:\n",
    "            print(\"\\n‚ö†Ô∏è Limited to 100 examples for demo. Remove limit for full evaluation.\")\n",
    "            break\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "output_path = PREDICTIONS_DIR / \"val_predictions.csv\"\n",
    "PREDICTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "predictions_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Predictions saved: {output_path}\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(min(3, len(predictions))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Note ID: {predictions[i]['note_id']}\")\n",
    "    print(f\"  Prediction: {predictions[i]['prediction'][:200]}...\")\n",
    "    print(f\"  Reference:  {predictions[i]['reference'][:200]}...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c4e5d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ ROUGE Evaluation\n",
    "\n",
    "**Expected Runtime:** ~30 seconds  \n",
    "**What this cell does:**\n",
    "- Computes ROUGE-1, ROUGE-2, ROUGE-L metrics\n",
    "- Compares model vs baseline (lead-k)\n",
    "- Displays results table\n",
    "\n",
    "**Metrics:**\n",
    "- **ROUGE-1:** Unigram overlap (word-level recall)\n",
    "- **ROUGE-2:** Bigram overlap (phrase-level matching)\n",
    "- **ROUGE-L:** Longest common subsequence (fluency)\n",
    "\n",
    "**Baseline:**\n",
    "- **Lead-K:** First K sentences of source as summary (simple baseline)\n",
    "\n",
    "**Common Issues:**\n",
    "- Low scores: Check if predictions are empty or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36843305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores\n",
    "from utils.metrics import RougeMetric, format_rouge_scores\n",
    "\n",
    "print(\"üìä Computing ROUGE scores...\\n\")\n",
    "\n",
    "# Load predictions\n",
    "predictions_path = PREDICTIONS_DIR / \"val_predictions.csv\"\n",
    "predictions_df = pd.read_csv(predictions_path)\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions_list = predictions_df['prediction'].tolist()\n",
    "references_list = predictions_df['reference'].tolist()\n",
    "\n",
    "# Compute ROUGE\n",
    "metric = RougeMetric()\n",
    "for pred, ref in zip(predictions_list, references_list):\n",
    "    metric.update(pred, ref)\n",
    "\n",
    "scores = metric.compute()\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"ROUGE Evaluation Results (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: Pointer-Generator with Coverage\")\n",
    "print(f\"Checkpoint: {CHECKPOINTS_DIR / 'best_model.pt'}\")\n",
    "print(f\"Examples evaluated: {len(predictions_list)}\\n\")\n",
    "\n",
    "print(format_rouge_scores(scores))\n",
    "\n",
    "# Create results table\n",
    "results_table = pd.DataFrame({\n",
    "    'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L'],\n",
    "    'Precision': [\n",
    "        scores['rouge1']['precision'],\n",
    "        scores['rouge2']['precision'],\n",
    "        scores['rougeL']['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        scores['rouge1']['recall'],\n",
    "        scores['rouge2']['recall'],\n",
    "        scores['rougeL']['recall']\n",
    "    ],\n",
    "    'F1': [\n",
    "        scores['rouge1']['f1'],\n",
    "        scores['rouge2']['f1'],\n",
    "        scores['rougeL']['f1']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + results_table.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "results_path = PREDICTIONS_DIR / \"rouge_results.csv\"\n",
    "results_table.to_csv(results_path, index=False)\n",
    "print(f\"\\n‚úì Results saved: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02357d4f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Baseline Comparison\n",
    "\n",
    "**Expected Runtime:** ~2 minutes  \n",
    "**What this cell does:**\n",
    "- Computes lead-k baseline (first K sentences)\n",
    "- Compares model performance vs baseline\n",
    "\n",
    "**Skip Condition:** `RUN_BASELINES = False` (use existing baseline results)\n",
    "\n",
    "**Common Issues:**\n",
    "- Baseline not found: Run `baselines.py` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "RUN_BASELINES = False  # Set True to recompute baselines\n",
    "\n",
    "# ========== CHECK EXISTING BASELINES ==========\n",
    "baseline_results_path = ARTIFACTS_DIR / \"baselines\" / \"val_results.csv\"\n",
    "\n",
    "if baseline_results_path.exists():\n",
    "    print(\"‚úì Baseline results found\\n\")\n",
    "    baseline_df = pd.read_csv(baseline_results_path)\n",
    "    print(baseline_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Baseline results not found. Set RUN_BASELINES = True.\")\n",
    "\n",
    "# ========== RUN BASELINES (if needed) ==========\n",
    "if RUN_BASELINES:\n",
    "    print(\"\\nüîÑ Computing baselines...\")\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, \"baselines.py\",\n",
    "         \"--tokenized_dir\", str(TOKENIZED_DIR),\n",
    "         \"--split\", \"val\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    else:\n",
    "        print(f\"‚úì Baselines saved to {ARTIFACTS_DIR / 'baselines'}\")\n",
    "\n",
    "# ========== COMPARISON ==========\n",
    "if baseline_results_path.exists() and (PREDICTIONS_DIR / \"rouge_results.csv\").exists():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Model vs Baseline Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    baseline_df = pd.read_csv(baseline_results_path)\n",
    "    model_df = pd.read_csv(PREDICTIONS_DIR / \"rouge_results.csv\")\n",
    "    \n",
    "    # Find lead-k baseline (typically best baseline)\n",
    "    lead_k = baseline_df[baseline_df['Method'].str.contains('lead', case=False)].iloc[0]\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Method': ['Lead-K Baseline', 'Pointer-Generator (Ours)'],\n",
    "        'ROUGE-1 F1': [lead_k['ROUGE-1 F1'], model_df[model_df['Metric'] == 'ROUGE-1']['F1'].values[0]],\n",
    "        'ROUGE-2 F1': [lead_k['ROUGE-2 F1'], model_df[model_df['Metric'] == 'ROUGE-2']['F1'].values[0]],\n",
    "        'ROUGE-L F1': [lead_k['ROUGE-L F1'], model_df[model_df['Metric'] == 'ROUGE-L']['F1'].values[0]]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + comparison.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894374b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Results Summary & Next Steps\n",
    "\n",
    "**Final Summary:**\n",
    "- ‚úÖ Model trained from scratch (no pretrained weights)\n",
    "- ‚úÖ Custom SentencePiece tokenizer trained on clinical text\n",
    "- ‚úÖ Hierarchical pointer-generator with coverage mechanism\n",
    "- ‚úÖ ROUGE evaluation on validation set\n",
    "- ‚úÖ All artifacts preserved and reusable\n",
    "\n",
    "**Key Files:**\n",
    "- Tokenizer: `artifacts/tokenizer/spm.model`\n",
    "- Best model: `artifacts/checkpoints/final_check/best_model.pt`\n",
    "- Predictions: `artifacts/predictions/val_predictions.csv`\n",
    "- Results: `artifacts/predictions/rouge_results.csv`\n",
    "\n",
    "**Next Steps:**\n",
    "1. **Error Analysis:** Inspect failed examples, identify common issues\n",
    "2. **Hyperparameter Tuning:** Adjust learning rate, beam size, coverage weight\n",
    "3. **Longer Training:** Current checkpoint at ~500-600 steps; try 10K+ steps\n",
    "4. **Test Set Evaluation:** Run `evaluate.py --split test` for final results\n",
    "5. **Deployment:** Export model for production inference\n",
    "\n",
    "**Troubleshooting:**\n",
    "- Low ROUGE: Try longer training, larger model, or better preprocessing\n",
    "- Repetitive summaries: Increase coverage weight in config\n",
    "- OOM errors: Reduce batch size, enable gradient checkpointing\n",
    "\n",
    "**Questions?** Check `README.md` or archived notebooks in `archive/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final checkpoint\n",
    "print(\"=\"*80)\n",
    "print(\"üéâ Pipeline Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úì Tokenizer: {TOKENIZER_DIR / 'spm.model'}\")\n",
    "print(f\"‚úì Model: {CHECKPOINTS_DIR / 'best_model.pt'}\")\n",
    "print(f\"‚úì Predictions: {PREDICTIONS_DIR / 'val_predictions.csv'}\")\n",
    "print(f\"‚úì Results: {PREDICTIONS_DIR / 'rouge_results.csv'}\")\n",
    "print(\"\\nAll artifacts preserved. Notebook can be rerun without retraining.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
