# Configuration for Resuming Training
# Must match the checkpoint's architecture

# Model Architecture (from checkpoint)
model:
  emb_dim: 192                  # Must match checkpoint
  hidden_dim: 384              # Must match checkpoint
  num_layers: 1                # Must match checkpoint
  dropout: 0.3                 # Dropout rate
  chunk_len: 256               # Tokens per chunk
  num_chunks: 6                # Number of chunks (max input = 256 * 6 = 1536)
  max_target_len: 256          # Maximum target sequence length
  coverage_lambda: 1.0         # Coverage loss weight
  pointer_gen: true            # Enable pointer-generator mechanism
  use_coverage: true           # Enable coverage mechanism
  bidirectional: true          # Use bidirectional LSTM for encoder

# Training
training:
  batch_size: 2                # Batch size per GPU
  grad_accum: 8                # Gradient accumulation steps (effective batch = 16)
  learning_rate: 0.0003        # Initial learning rate
  max_steps: 50000             # Maximum training steps
  warmup_steps: 1000           # Learning rate warmup steps
  fp16: true                   # Use mixed precision training
  clip_grad: 1.0               # Gradient clipping norm
  save_every: 300              # Save checkpoint every N steps
  eval_every: 500              # Evaluate every N steps
  log_every: 50                # Log training metrics every N steps
  patience: 10                 # Early stopping patience (evals without improvement)
  seed: 42                     # Random seed

# Optimizer
optimizer:
  type: adam                   # Optimizer type: adam, sgd
  betas: [0.9, 0.999]          # Adam beta parameters
  eps: 1.0e-8                  # Adam epsilon
  weight_decay: 0.0001         # L2 regularization

# Learning Rate Scheduler
scheduler:
  type: linear_warmup_decay    # linear_warmup_decay, cosine, step
  decay_steps: 45000           # Steps to decay LR to 0 (after warmup)

# Decoding
decoding:
  beam_size: 4                 # Beam search beam size
  min_length: 50               # Minimum generation length
  max_length: 256              # Maximum generation length
  length_penalty: 1.0          # Length penalty factor

# Data
data:
  pad_id: 0                    # Padding token ID
  unk_id: 1                    # Unknown token ID
  bos_id: 2                    # Beginning of sequence token ID
  eos_id: 3                    # End of sequence token ID
  vocab_size: 16000            # Vocabulary size

# Paths
paths:
  tokenizer_dir: 'artifacts/tokenizer'
  checkpoint_dir: 'artifacts/checkpoints'
  log_dir: 'artifacts/logs'
  data_dir: 'data'
