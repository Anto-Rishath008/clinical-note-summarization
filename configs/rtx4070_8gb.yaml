# Configuration for RTX 4070 8GB Laptop GPU
# Memory-optimized settings for full dataset training

# Model Architecture
model:
  emb_dim: 128                  # Further reduced embedding dimension
  hidden_dim: 256              # Further reduced LSTM hidden dimension
  num_layers: 2                # Number of LSTM layers
  dropout: 0.3                 # Dropout rate
  chunk_len: 128               # Further reduced tokens per chunk
  num_chunks: 6                # Reduced number of chunks (max input = 128 * 6 = 768)
  max_target_len: 192          # Further reduced maximum target sequence length
  coverage_lambda: 0.0         # Coverage loss weight (disabled initially for stability)
  pointer_gen: true            # Enable pointer-generator mechanism
  use_coverage: false          # Coverage disabled initially - enable after model learns
  bidirectional: true          # Use bidirectional LSTM for encoder

# Training
training:
  batch_size: 1                # Reduced batch size for 8GB VRAM
  grad_accum: 16               # Increased gradient accumulation (effective batch = 16)
  learning_rate: 0.0003        # Initial learning rate
  max_steps: 50000             # Maximum training steps
  warmup_steps: 1000           # Learning rate warmup steps
  fp16: true                   # Use mixed precision training
  clip_grad: 1.0               # Gradient clipping norm
  save_every: 500              # Save checkpoint every N steps
  eval_every: 500              # Evaluate every N steps
  log_every: 50                # Log training metrics every N steps
  patience: 10                 # Early stopping patience (evals without improvement)
  seed: 42                     # Random seed

# Optimizer
optimizer:
  type: adam                   # Optimizer type: adam, sgd
  betas: [0.9, 0.999]          # Adam beta parameters
  eps: 1.0e-8                  # Adam epsilon
  weight_decay: 0.0001         # L2 regularization

# Learning Rate Scheduler
scheduler:
  type: linear_warmup_decay    # linear_warmup_decay, cosine, step
  decay_steps: 45000           # Steps to decay LR to 0 (after warmup)

# Decoding
decoding:
  beam_size: 4                 # Beam search beam size
  min_length: 50               # Minimum generation length
  max_length: 256              # Maximum generation length
  length_penalty: 1.0          # Length penalty factor
  no_repeat_ngram: 3           # Block repeating n-grams (0 = disabled)
  block_trigrams: true         # Block trigram repetition

# Data
data:
  train_split: 0.8             # Training split ratio
  val_split: 0.1               # Validation split ratio
  test_split: 0.1              # Test split ratio
  vocab_size: 16000            # SentencePiece vocabulary size
  unk_id: 0                    # Unknown token ID
  bos_id: 1                    # Begin-of-sequence token ID
  eos_id: 2                    # End-of-sequence token ID
  pad_id: 3                    # Padding token ID

# Paths
paths:
  tokenizer_dir: artifacts/tokenizer
  checkpoint_dir: artifacts/checkpoints
  log_dir: artifacts/logs
  predictions_dir: artifacts/predictions
