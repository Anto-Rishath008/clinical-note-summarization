# Clinical Note Summarization - From-Scratch Training

A PyTorch implementation of a **Pointer-Generator Network with Hierarchical Encoding** for automatic summarization of clinical notes into hospital course summaries.

## âš ï¸ IMPORTANT: From-Scratch Training Only

**This project trains models from random initialization (NO pretrained models, NO PEFT/LoRA).**

Goal: Beat Lead-150 baseline using disciplined from-scratch training with sanity checks.

**âš ï¸ Dataset**: This repository contains **NO patient data or trained models**. The MIMIC-IV-BHC dataset requires credentialed access from PhysioNet.

---

## ğŸš€ Quick Start (Windows PowerShell)

### Prerequisites
```powershell
# Create virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Prepare tokenized data (after obtaining MIMIC-IV-BHC dataset)
python preprocess_data.py
```

### Essential Commands

```powershell
# 1. Overfit test (MANDATORY - verifies model can learn)
python scripts\overfit_test.py --config configs\stage1_fromscratch.yaml

# 2. Establish baseline target
python scripts\lead_baseline.py --n_tokens 150

# 3. Train from scratch
python train.py --config configs\stage1_fromscratch.yaml

# 4. Evaluate trained model
python evaluate.py --checkpoint artifacts\checkpoints\best_model.pt --split test

# 5. Decode sanity check (verify generation quality)
python scripts\decode_sanity.py --checkpoint artifacts\checkpoints\best_model.pt
```

--- ## ğŸ“ Repository Structure

```
clinical-note-summarization/
â”œâ”€â”€ train.py                     # Main training script (FP16, gradient accumulation)
â”œâ”€â”€ evaluate.py                  # Evaluation with ROUGE metrics
â”œâ”€â”€ preprocess_data.py           # Data tokenization pipeline
â”œâ”€â”€ inspect_dataset.py           # Dataset inspection utility
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ core.py                  # Model implementation (1,200+ lines)
â”‚                                  - Hierarchical BiLSTM encoder
â”‚                                  - Pointer-Generator decoder
â”‚                                  - Attention mechanisms (Additive + Coverage)
â”‚                                  - Beam search decoder
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ stage1_fromscratch.yaml  # From-scratch training (RECOMMENDED)
â”‚   â”œâ”€â”€ stage1_debug.yaml        # Quick debugging config
â”‚   â”œâ”€â”€ default.yaml             # Standard configuration
â”‚   â”œâ”€â”€ resume_config.yaml       # Resume training
â”‚   â”œâ”€â”€ rtx4070_8gb.yaml         # Memory-optimized (8GB VRAM)
â”‚   â””â”€â”€ _deprecated/             # Pretrained model configs (not used)
â”‚       â”œâ”€â”€ hf_baseline.yaml
â”‚       â””â”€â”€ hf_experiments/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ overfit_test.py          # Overfit sanity check (200 samples)
â”‚   â”œâ”€â”€ lead_baseline.py         # Lead-N baseline evaluation
â”‚   â”œâ”€â”€ baseline_eval.py         # Baseline comparison
â”‚   â”œâ”€â”€ decode_sanity.py         # Generation quality analysis
â”‚   â”œâ”€â”€ eval_pipeline.py         # Comprehensive evaluation
â”‚   â”œâ”€â”€ clean_repo.ps1           # Remove all artifacts (PowerShell)
â”‚   â”œâ”€â”€ prepare_data.ps1         # Dataset preparation script
â”‚   â”œâ”€â”€ train_sanity.ps1         # Quick sanity check training
â”‚   â””â”€â”€ _deprecated/             # Pretrained model scripts (not used)
â”‚       â”œâ”€â”€ _DEPRECATED_hf_train.py
â”‚       â””â”€â”€ _DEPRECATED_hf_eval.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ results.csv              # Experiment tracking table
â”‚   â”œâ”€â”€ baseline_results.csv     # Lead-K baseline results
â”‚   â””â”€â”€ decode_sanity_results.csv # Decode quality metrics
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ implementation_summary.md # Implementation details
â”‚   â”œâ”€â”€ rouge_optimization_plan.md # Optimization strategies
â”‚   â””â”€â”€ archive/                  # Archived process documentation
â”‚       â”œâ”€â”€ CLEANUP_SUMMARY.md
â”‚       â”œâ”€â”€ QUICK_REFERENCE.md
â”‚       â”œâ”€â”€ STAGE1_*.md
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/ (âš ï¸ Gitignored - PHI Protected)
â”‚   â”œâ”€â”€ README.md                # Data acquisition instructions
â”‚   â”œâ”€â”€ sample_data.json         # 2 non-sensitive examples
â”‚   â””â”€â”€ tokenized/               # Preprocessed data (generated)
â”‚
â”œâ”€â”€ artifacts/ (âš ï¸ Gitignored - Generated files)
â”‚   â”œâ”€â”€ README.md                # Artifacts documentation
â”‚   â”œâ”€â”€ checkpoints/             # Model weights (*.pt files)
â”‚   â”œâ”€â”€ logs/                    # Training logs
â”‚   â””â”€â”€ tokenizer/               # SentencePiece models
â”‚
â””â”€â”€ reports/
    â””â”€â”€ README.md                # Small result summaries
```

### Key Directories

| Directory | Purpose | Git Tracked |
|-----------|---------|-------------|
| `src/` | Source code | âœ… Yes |
| `configs/` | YAML configurations | âœ… Yes |
| `scripts/` | Utility scripts | âœ… Yes |
| `experiments/` | Small CSV results | âœ… Yes |
| `docs/` | Documentation | âœ… Yes |
| `data/` | Raw/processed datasets | âŒ **No** (PHI protected) |
| `artifacts/` | Model checkpoints, logs | âŒ **No** (10+ GB) |

**âš ï¸ Important**: Artifacts and data are **never committed** to version control. They are regenerated during training.

---

## ğŸ“‹ Prerequisites

- **Python**: 3.10+ recommended
- **PyTorch**: 2.0+ with CUDA support (for GPU training)
- **Hardware**: 
  - Minimum: 8GB VRAM GPU (RTX 4070 or better)
  - Recommended: 12GB+ VRAM GPU
  - CPU training possible but very slow (~100x slower)
- **Storage**: 15-20 GB free space (datasets + artifacts)

---

## ğŸ“¦ Model Architecture

This project implements a neural sequence-to-sequence model that:
- Processes long clinical notes (up to 2,048 tokens) using hierarchical chunked encoding
- Generates concise summaries using a pointer-generator mechanism
- Handles medical terminology through a copy mechanism
- Prevents repetition with coverage attention

See [docs/implementation_summary.md](docs/implementation_summary.md) for detailed architecture information.

---

## ğŸ“š Documentation

- **Implementation Details**: [docs/implementation_summary.md](docs/implementation_summary.md)
- **Optimization Strategies**: [docs/rouge_optimization_plan.md](docs/rouge_optimization_plan.md)
- **Archived Process Docs**: [docs/archive/](docs/archive/)

---

## âš ï¸ Deprecated Components

The following components are **NOT USED** in this from-scratch training project:

- `configs/_deprecated/hf_baseline.yaml` - HuggingFace BART fine-tuning configs
- `configs/_deprecated/hf_experiments/` - HuggingFace experiment configs
- `scripts/_deprecated/_DEPRECATED_hf_train.py` - Pretrained model training
- `scripts/_deprecated/_DEPRECATED_hf_eval.py` - Pretrained model evaluation

These files are kept for reference but are not part of the active codebase.

---
## ğŸ”¬ Training Workflow

1. **Sanity Check**: Run overfit test to verify model can learn (5-10 minutes)
2. **Baseline**: Establish Lead-150 baseline target
3. **Training**: Train from scratch (40-60 hours on RTX 4070)
4. **Evaluation**: Compare model performance against baseline

---

## ğŸ§ª Development & Testing

```powershell
# Clean all generated artifacts
.\scripts\clean_repo.ps1

# Quick sanity check (5-10 min)
.\scripts\train_sanity.ps1

# Inspect preprocessed dataset
python inspect_dataset.py
```

---

## ğŸ“ˆ Results & Experiments

Training results and metrics are tracked in:
- `experiments/results.csv` - Main experiment tracking
- `experiments/baseline_results.csv` - Baseline comparisons
- `experiments/decode_sanity_results.csv` - Generation quality

---

## ğŸ¤ Contributing

This is an academic project. For questions or collaboration, please open an issue.

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- MIMIC-IV-BHC dataset from PhysioNet
- Pointer-Generator architecture based on See et al. (2017)
- Hierarchical encoding inspired by clinical NLP literature
## ğŸ“Š Data

### Obtaining the Dataset

This project uses the **MIMIC-IV-BHC** dataset from PhysioNet:

1. **Request Access**: https://physionet.org/content/mimic-iv-bhc/1.2.0/
   - Requires CITI training completion
   - Sign data use agreement
   - **HIPAA compliance required**

2. **Download**: After approval, download `mimic-iv-bhc.csv` (2.6 GB)

3. **Preprocess**:
   ```powershell
   python preprocess_data.py
   ```

**âš ï¸ CRITICAL**: Do NOT commit patient data to version control. Follow all PhysioNet data use agreements.

### Dataset Statistics
- **Total Examples**: 270,034 clinical note-summary pairs
- **Splits**: 80% train / 10% validation / 10% test

---

## ğŸ”§ Configuration

Edit YAML files in `configs/` to customize training:

**Key Parameters**:
- `model.emb_dim` - Embedding dimension (192)
- `model.hidden_dim` - LSTM hidden size (384-512)
- `model.num_layers` - LSTM layers (1-2)
- `training.batch_size` - Per-GPU batch size (1-2)
- `training.grad_accum` - Gradient accumulation steps (8-16)
- `training.learning_rate` - Initial learning rate (0.0003)
- `training.max_steps` - Training steps (50000)

See `configs/default.yaml` for full configuration options.

---

## ğŸ“ˆ Expected Performance

**Baselines (Lead-K extractive)**:
- Lead-100: ROUGE-L 0.1186
- Lead-150: ROUGE-L 0.1510 (target to beat)

**From-Scratch Training Goals**:
- âœ… Beat Lead-150 baseline (ROUGE-L > 0.15)
- âœ… Achieve ROUGE-L > 0.25 (acceptable performance)
- ğŸ¯ Target ROUGE-L > 0.30 (strong performance)

Training time: ~40-60 hours on RTX 4070 12GB

---

## ğŸ§¹ Cleanup

Remove generated artifacts to reclaim disk space:

```powershell
# Preview what will be deleted
.\scripts\clean_repo.ps1 -DryRun

# Clean all artifacts (keeps .venv)
.\scripts\clean_repo.ps1
```

All artifacts can be regenerated by rerunning preprocessing and training.

---

## ğŸ¤ Contributing

This is an academic research project. For questions or collaboration, open a GitHub issue.

**Areas for Improvement**:
- Add unit tests
- Implement additional metrics (BLEU, BERTScore)
- Add Docker support
- Create inference API

---

## ğŸ“„ License

MIT License - See LICENSE file for details

**âš ï¸ Data Privacy**: Never commit patient data or trained models to version control.

---

## ğŸ™ Acknowledgments

- MIMIC-IV-BHC dataset from PhysioNet
- Pointer-Generator architecture based on See et al. (2017)
- Hierarchical encoding inspired by clinical NLP literature

---

## ğŸ“§ Contact

For questions or issues:
- Open a GitHub issue
- Repository: https://github.com/Anto-Rishath008/clinical-note-summarization
